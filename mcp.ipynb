{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2568ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import subprocess, time, json, urllib.request\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa685010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient  \n",
    "from langchain.agents import create_agent\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain.agents import create_agent\n",
    "import asyncio\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "client = MultiServerMCPClient({\n",
    "    # Local custom tool\n",
    "    \"JobTools\": {\n",
    "        \"transport\": \"stdio\",  # Local subprocess communication\n",
    "        \"command\": \"/home/tom/apps/cache/python-envs/ML/bin/python\",\n",
    "        \"args\": [\"/home/tom/WD/langchain1.0/mcp-tools/wrapper.py\"],\n",
    "    },\n",
    "\n",
    "    # Playwright MCP server (via npx)\n",
    "    \"playwright\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": \"npx\",\n",
    "        \"args\": [\"@playwright/mcp@latest\",],\n",
    "    },\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05cdfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN WITH llama-server -m Holo1.5-3B.Q4_K_S.gguf --jinja\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Connect to your local llama.cpp server\n",
    "llm = ChatOpenAI(\n",
    "    model=\"Holo1.5-3B.Q4_K_S\",         \n",
    "    api_key=\"EMPTY\",                    \n",
    "    base_url=\"http://127.0.0.1:8080/v1\",\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0882c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = await client.get_tools()  \n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from typing import List, Literal\n",
    "\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade5ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_tools(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the workflow from StateGraph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# add a node named LLM, with call_model function. This node uses an LLM to make decisions based on the input given\n",
    "workflow.add_node(\"LLM\", call_model)\n",
    "\n",
    "# Our workflow starts with the LLM node\n",
    "workflow.add_edge(START, \"LLM\")\n",
    "\n",
    "# Add a tools node\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add a conditional edge from LLM to call_tools function. It can go tools node or end depending on the output of the LLM. \n",
    "workflow.add_conditional_edges(\"LLM\", call_tools)\n",
    "\n",
    "# tools node sends the information back to the LLM\n",
    "workflow.add_edge(\"tools\", \"LLM\")\n",
    "\n",
    "agent = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in agent.astream(\n",
    "    {\"messages\": [(\"user\", \"go to indeed and search for ml engineering roles and apply to one using the tools you have, make cover letter and resume\")]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
